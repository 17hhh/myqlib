{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0912,  0.0242, -0.0067,  ...,  0.0329,  0.0298, -0.1147],\n",
      "         [ 0.0216,  0.0816, -0.0537,  ...,  0.1017,  0.0418, -0.2685],\n",
      "         [-0.0189,  0.0255,  0.0096,  ...,  0.0613,  0.0503, -0.2193],\n",
      "         [ 0.0119,  0.0492, -0.0382,  ..., -0.0119,  0.0535, -0.2352],\n",
      "         [ 0.0832,  0.0399,  0.0035,  ...,  0.0552,  0.0099, -0.1576]],\n",
      "\n",
      "        [[ 0.1606, -0.0175,  0.3246,  ...,  0.1131,  0.0707,  0.3990],\n",
      "         [ 0.1115,  0.0606,  0.1783,  ...,  0.1294, -0.0480,  0.2024],\n",
      "         [ 0.1033, -0.0319,  0.3518,  ...,  0.0861, -0.0696,  0.4981],\n",
      "         [ 0.0713, -0.0376,  0.2406,  ...,  0.0457, -0.0945,  0.3883],\n",
      "         [ 0.1189, -0.0490,  0.3508,  ...,  0.0787, -0.0059,  0.3268]],\n",
      "\n",
      "        [[ 0.2070, -0.0687,  0.2748,  ..., -0.0871,  0.0087,  0.0167],\n",
      "         [ 0.1150,  0.0580,  0.1072,  ..., -0.0395,  0.0171,  0.0484],\n",
      "         [ 0.0861, -0.0493,  0.2033,  ..., -0.0132,  0.0346,  0.0041],\n",
      "         [ 0.1844, -0.0331,  0.1966,  ..., -0.0746,  0.0686, -0.0212],\n",
      "         [ 0.2897, -0.0871,  0.1860,  ..., -0.1042,  0.0776,  0.0006]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1492,  0.0150, -0.0039,  ...,  0.1749, -0.0229,  0.2596],\n",
      "         [ 0.1577,  0.0176,  0.0872,  ...,  0.0186, -0.0231,  0.1583],\n",
      "         [ 0.0904,  0.0052,  0.0482,  ...,  0.1909, -0.0911,  0.2326],\n",
      "         [ 0.1326,  0.0225,  0.0970,  ...,  0.1200, -0.1809,  0.1536],\n",
      "         [ 0.0987, -0.0120,  0.1646,  ...,  0.1799, -0.1028,  0.1670]],\n",
      "\n",
      "        [[-0.1874,  0.2511, -0.0588,  ..., -0.0686,  0.1119,  0.1795],\n",
      "         [-0.2017,  0.2359, -0.0057,  ..., -0.1317,  0.0062,  0.0313],\n",
      "         [-0.2465,  0.3300, -0.0529,  ..., -0.1262,  0.1903,  0.1398],\n",
      "         [-0.2691,  0.1839, -0.0082,  ..., -0.0414,  0.0857,  0.0010],\n",
      "         [-0.2110,  0.2079,  0.0033,  ...,  0.0117,  0.0942,  0.0734]],\n",
      "\n",
      "        [[-0.2539,  0.0386,  0.0180,  ...,  0.0875,  0.1147,  0.0724],\n",
      "         [-0.1458,  0.1057, -0.0570,  ...,  0.1592,  0.0336,  0.0485],\n",
      "         [-0.1704,  0.0829,  0.0534,  ...,  0.1709,  0.1870,  0.1433],\n",
      "         [-0.4157,  0.2244,  0.0652,  ...,  0.2291,  0.1284,  0.1432],\n",
      "         [-0.3029, -0.0239, -0.0148,  ...,  0.0447,  0.1358,  0.0566]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "seed = 32\n",
    "np.random.seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "from transformerModule import PositionalEncoding, MultiHeadAttention, MultiHeadedAttention\n",
    "\n",
    "input = torch.randn(20,5,512)\n",
    "mul_att1 = MultiHeadAttention(2,512,0.1)\n",
    "mul_att_out1 = mul_att1.forward(input.float())\n",
    "# 模块参数\n",
    "# print(mul_att1.state_dict())\n",
    "# for name1, param1 in mul_att1.named_parameters():\n",
    "#     print(name1, param1)\n",
    "# 模块结果\n",
    "print(mul_att_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0912,  0.0242, -0.0067,  ...,  0.0329,  0.0298, -0.1147],\n",
      "         [ 0.0216,  0.0816, -0.0537,  ...,  0.1017,  0.0418, -0.2685],\n",
      "         [-0.0189,  0.0255,  0.0096,  ...,  0.0613,  0.0503, -0.2193],\n",
      "         [ 0.0119,  0.0492, -0.0382,  ..., -0.0119,  0.0535, -0.2352],\n",
      "         [ 0.0832,  0.0399,  0.0035,  ...,  0.0552,  0.0099, -0.1576]],\n",
      "\n",
      "        [[ 0.1606, -0.0175,  0.3246,  ...,  0.1131,  0.0707,  0.3990],\n",
      "         [ 0.1115,  0.0606,  0.1783,  ...,  0.1294, -0.0480,  0.2024],\n",
      "         [ 0.1033, -0.0319,  0.3518,  ...,  0.0861, -0.0696,  0.4981],\n",
      "         [ 0.0713, -0.0376,  0.2406,  ...,  0.0457, -0.0945,  0.3883],\n",
      "         [ 0.1189, -0.0490,  0.3508,  ...,  0.0787, -0.0059,  0.3268]],\n",
      "\n",
      "        [[ 0.2070, -0.0687,  0.2748,  ..., -0.0871,  0.0087,  0.0167],\n",
      "         [ 0.1150,  0.0580,  0.1072,  ..., -0.0395,  0.0171,  0.0484],\n",
      "         [ 0.0861, -0.0493,  0.2033,  ..., -0.0132,  0.0346,  0.0041],\n",
      "         [ 0.1844, -0.0331,  0.1966,  ..., -0.0746,  0.0686, -0.0212],\n",
      "         [ 0.2897, -0.0871,  0.1860,  ..., -0.1042,  0.0776,  0.0006]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1492,  0.0150, -0.0039,  ...,  0.1749, -0.0229,  0.2596],\n",
      "         [ 0.1577,  0.0176,  0.0872,  ...,  0.0186, -0.0231,  0.1583],\n",
      "         [ 0.0904,  0.0052,  0.0482,  ...,  0.1909, -0.0911,  0.2326],\n",
      "         [ 0.1326,  0.0225,  0.0970,  ...,  0.1200, -0.1809,  0.1536],\n",
      "         [ 0.0987, -0.0120,  0.1646,  ...,  0.1799, -0.1028,  0.1670]],\n",
      "\n",
      "        [[-0.1874,  0.2511, -0.0588,  ..., -0.0686,  0.1119,  0.1795],\n",
      "         [-0.2017,  0.2359, -0.0057,  ..., -0.1317,  0.0062,  0.0313],\n",
      "         [-0.2465,  0.3300, -0.0529,  ..., -0.1262,  0.1903,  0.1398],\n",
      "         [-0.2691,  0.1839, -0.0082,  ..., -0.0414,  0.0857,  0.0010],\n",
      "         [-0.2110,  0.2079,  0.0033,  ...,  0.0117,  0.0942,  0.0734]],\n",
      "\n",
      "        [[-0.2539,  0.0386,  0.0180,  ...,  0.0875,  0.1147,  0.0724],\n",
      "         [-0.1458,  0.1057, -0.0570,  ...,  0.1592,  0.0336,  0.0485],\n",
      "         [-0.1704,  0.0829,  0.0534,  ...,  0.1709,  0.1870,  0.1433],\n",
      "         [-0.4157,  0.2244,  0.0652,  ...,  0.2291,  0.1284,  0.1432],\n",
      "         [-0.3029, -0.0239, -0.0148,  ...,  0.0447,  0.1358,  0.0566]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "seed = 32\n",
    "np.random.seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "from transformerModule import PositionalEncoding, MultiHeadAttention, MultiHeadedAttention\n",
    "\n",
    "input = torch.randn(20,5,512)\n",
    "mul_att1 = MultiHeadedAttention(2,512,0.1)\n",
    "mul_att_out1 = mul_att1.forward(input.float(), input.float(), input.float())\n",
    "# 模块参数\n",
    "# print(mul_att1.state_dict())\n",
    "# for name1, param1 in mul_att1.named_parameters():\n",
    "#     print(name1, param1)\n",
    "# 模块结果\n",
    "print(mul_att_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "seed = 32\n",
    "np.random.seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input = torch.randn(20,5)\n",
    "\n",
    "linear1 = nn.Linear(5,2)\n",
    "param1 = linear1.state_dict()\n",
    "\n",
    "seed = 32\n",
    "np.random.seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "linear2 = nn.Linear(5,2)\n",
    "# linear2.load_state_dict(param1)\n",
    "\n",
    "output1 = linear1(input)\n",
    "output2 = linear2(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "seed = 32\n",
    "np.random.seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input = torch.randn(20,5)\n",
    "\n",
    "\n",
    "\n",
    "linear2 = nn.Linear(5,2)\n",
    "\n",
    "\n",
    "output2 = linear2(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8670,  0.2194],\n",
       "        [ 0.4261, -0.2586],\n",
       "        [ 0.2927,  0.3177],\n",
       "        [ 0.3878, -0.0392],\n",
       "        [ 0.1735,  0.5971],\n",
       "        [ 0.2066,  0.5253],\n",
       "        [ 0.1987, -0.3484],\n",
       "        [ 0.1141, -0.8061],\n",
       "        [-0.2912, -0.6373],\n",
       "        [ 0.7464,  0.2669],\n",
       "        [ 0.9938,  0.1806],\n",
       "        [ 0.3014, -0.4113],\n",
       "        [ 0.2747, -0.0263],\n",
       "        [ 0.4468,  0.0230],\n",
       "        [ 0.3481, -0.5977],\n",
       "        [ 0.1460, -0.1030],\n",
       "        [ 0.0869, -0.0359],\n",
       "        [ 0.0924, -0.6097],\n",
       "        [-0.0361, -0.1957],\n",
       "        [ 0.7113,  0.3588]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8670,  0.2194],\n",
       "        [ 0.4261, -0.2586],\n",
       "        [ 0.2927,  0.3177],\n",
       "        [ 0.3878, -0.0392],\n",
       "        [ 0.1735,  0.5971],\n",
       "        [ 0.2066,  0.5253],\n",
       "        [ 0.1987, -0.3484],\n",
       "        [ 0.1141, -0.8061],\n",
       "        [-0.2912, -0.6373],\n",
       "        [ 0.7464,  0.2669],\n",
       "        [ 0.9938,  0.1806],\n",
       "        [ 0.3014, -0.4113],\n",
       "        [ 0.2747, -0.0263],\n",
       "        [ 0.4468,  0.0230],\n",
       "        [ 0.3481, -0.5977],\n",
       "        [ 0.1460, -0.1030],\n",
       "        [ 0.0869, -0.0359],\n",
       "        [ 0.0924, -0.6097],\n",
       "        [-0.0361, -0.1957],\n",
       "        [ 0.7113,  0.3588]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
